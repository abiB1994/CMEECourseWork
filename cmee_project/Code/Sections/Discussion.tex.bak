\documentclass[../../Paper.tex]{subfiles}
    
\begin{document}

All classification models achieved \textgreater 88\% training accuracy, with testing accuracy ranging from 31\% to 94\%. These accuracies are higher than both random chance and predicted human accuracy. Four out of the proposed hypotheses were supported by the data collected; hypothesis 4 (Normalised images will improve model accuracies) was not supported by the data obtained. Furthermore, a website containing some of these models was also built to allow others to test their own images using the models and to allow people to use these techniques in future research. 


\subsection*{Multi-stress treatments}

The results obtained from the multi-stress classifiers support hypothesis 1; both machine learning algorithms will be able to correctly classify the images with a training accuracy of \textgreater 85\%. Several studies have looked into classifying simple treatment images (see: \cite{baranowski_hyperspectral_2015,mohanty_using_2016,sladojevic_deep_2016,singh_detection_2017}) with classification accuracies reaching up to 99.35\%; Mohanty \textit{et al} (2016) note that their study was made harder by using a 38 class dataset, consisting of both crop species and disease, but that ultimately their CNNs still achieved accuracies of \textgreater 85\%. 

Despite numerous studies looking into simple stresses, and achieving high classification accuracies, there are no studies clearly documenting multi-stress classification experiments. This is a vital subject area, as in the real world plants are routinely subjected to various combinations of both biotic and abiotic stresses; it also being noted that combination stresses cannot be identified by extrapolating from individual stresses (\cite{rizhsky_combined_2002}).  Due to this, we believe that our results will be extremely significant both in the field and in future experiments, by allowing individuals to identfy multiple stessors that may be impacting upon their plants health. Knowing the impact that both biotic and abiotic factors have on plant growth, particularly when combined, will become extremely useful in disease identification as well as future treatment measures. 

Validation accuracies for models trained on datasets 1-3 were obtained from a mixed test set; several images were collected from various datasets. This may partly explain the lower testing accuracy (Table 2); despite this, the results obtained are indicative of a good overall model, with the potential to be used in future treatment studies. A Website was created containing the models created, allowing individuals to classify their own images. We aim to develop this into a fully functioning application to be used in future studies, whilst also allowing other users to add improvements and further train the models on new datasets.

Despite the obvious benefits of CNNs in comparison to RFs, classification results were almost identical (Table 2); the prediction certainties of true positive test results (Figure 5a) show that CNNs produce a significantly higher percentage certainty in comparison to RFs. This then allows support for hypothesis 5; CNNs are better classifiers in comparison to RFs. This has already been noted in several studies, with Baranowski \textit{et al} (2015) noting a classification rate of 55\% for a RF model, and 80\% for a Back-Propagation NN. With many ways to interpret a models output, it can often be difficult to settle on one statistic; it is believed that by producing several model comparison statistics, a more general view of the model can be obtained. By calculating percentage certainties, it can be better ascertained how an individual will respond to a classification accuracy; if two models correctly classify an unknown image (both to the models and the individual classifying the image), but one returns the prediction with a 25\% certainty, and the other with a 98\% certainty, the individual is far more likely to trust the later certainty. Our results favour CNNs, despite their increased requirements in comparison to RFs; future studies could build on previous CNNs, reducing build time as well as training time.

The use of colour-pass filters increased the models accuracy (Table 2); hypothesis 3 was thereby supported by this study. Colour-pass filters block out their colour specific wavelengths, and have been used before in an ecological classification survey (\cite{knoth_unmanned_2013}). By using them in classification studies, the object in question can come into focus whilst also allowing certain aspects of the object to become more apparent, as shown in Figure 2. This is likely due to metamerism, whereby different colours appear the same under certain conditions, with the colours in question referred to as metamers. By taking photographs of the data under various colour-pass filters, we can capture a range of colour information, with a higher likelihood of capturing the true colour of the dataset.

Due to the price and accessibility of colour-pass filters in comparison to Hyperspectral imaging cameras, as well as the results obtained from this study, we recommend that colour-pass filters be included in more classification studies to allow for greater data to be brought out of a dataset. Due to the ease of applying a colour-pass filter to a dataset, this method will also allow for the wider botanical and agricultural community to apply these techniques in future research. 

Training and testing on a normalised dataset resulted in models of a lower accuracy than identical models trained on non-normalised images; normalised images appeared to have a negative effect on classification rates, thus resulting in the rejection of hypothesis 4. It is possible that the process of normalising an image distorts the feature space of the image, thus making it harder for a model to classify. It is also possible that the method used to normalise the images was not sufficient; future studies should look into a wider array of normalising approaches, so as to ensure sufficient evidence has been obtained before making any compelling statements. 


\subsubsection*{Duration Infection}

The results of both models trained on the duration infection dataset support hypothesis 2;  RFs and CNNs will be able to correctly classify the images with a training accuracy of \textgreater 85\%. Another study, conducted by Zhu \textit{et al} 2016, also looked into classifying infection duration images; tobacco leaves were infected with Tobacco mosaic virus (TMV), and images were taken at 2 days post infection (DPI), 4DPI and 6DPI, along with healthy tobacco leaf images. Classification accuracies of up to 95\% were established, using hyperspectral images and a backp-propagation NN. Their study noted that is it hyperspectral imaging that has the potential to aid presymptomatic disease detection; our results indicate that even simple RGB images have the ability to advance detection. We also note that by 3 days (72 hpi), leaves were severely infected, and so believe that classifying many hours before this stage is essential if the crop is to be treated. Being able to classify between healthy and up to 72hpi leaves will greatly improve presymptomatic treatment; applying this technique in the field will allow for more precise pesticide and insecticide application, which is beneficial to both the community and the environment.

As in multi-treatment classification, the results for both models were almost identical (Table 2), whilst the prediction certainties (Figure 5b) show that CNNs produce a significantly higher percentage certainty in comparison to RFs. This then allows for further support for hypothesis 5. 


\subsection*{Pixel Intensities}

Pixel intensities for both datasets show that each treatment was significantly different to all other treatments within that dataset (Figures 7 and 8); this allows for further support of the classifiers built and the hypotheses supported, by acknowledging that they are classifying on relatively discrete and distinct treatments.

Appendix figures 1 and 2 show a more detailed view of the pixel intensities for each treatment. Two clear patterns are observed in Appendix Figure 1; the blue channel in the infection duration study significantly decreases as time increase, whilst the green and red channel significantly increase at 72hpi. This is likely an indication of ongioing infection, with the overall leaf colour changing from a green in 10hpi to a yellow/brown in 72hpi. Between 34 and 72hpi it is likely that chlorosis began, thus resulting in an increase in intensity in the red and green channels in the 72hpi treatment. 

\end{document}
   
   